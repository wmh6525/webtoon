{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "commentData = pd.read_csv('comment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 글자정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_origin = commentData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어 comment 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 글자 확인\n",
    "pd.set_option('display.max_rows', None)\n",
    "tmp = commentData['comment'].replace('[^a-zA-Z]','',regex=True)\n",
    "tmp[tmp.str.contains('[a-zA-Z]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어로 표현된 감정표현 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bb -> 굿, zz -> ㅋㅋ\n",
    "commentData['comment'] = commentData['comment'].str.replace(\"Good\",\"굿\")\n",
    "commentData['comment'] = commentData['comment'].str.replace(\"bb+\",\"굿\")\n",
    "commentData['comment'] = commentData['comment'].str.replace(\"zz+\",\"ㅋㅋ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 나머지 영어 삭제\n",
    "commentData['comment'] = commentData['comment'].replace('[a-zA-Z]','',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이모티콘 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이모티콘 대체\n",
    "commentData['comment'] = commentData['comment'].str.replace('[🙃🤣😆😀😊😄🤭😁😂]+','ㅋㅋ',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[😢😭🥺]+','ㅠㅠ',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[🔥👊💪]+','파이팅',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[👍🏻👍]+','굿',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[🎊🎉✨👏🥳💐]+','축하',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[💟♡♥️❤❤️💓💕💖💗💘💙💚💛💜💝💞😍😘😻🤍🤎🥰🧡😚💋]+','❤️',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[☆★⭐]+','별',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[🍪]+','쿠키',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('[🙏]+',\"제발\",regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자음 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자음 정리\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄷㄷ+','덜덜',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅎㅇㅌ','파이팅',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅇㅈ','인정',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄹㅈ','인정',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅂㄷ','부들',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅁㅇㅁㅇ','뭐야뭐야',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄹㅈㄷ','레전드',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄱㅇㅇ','귀여워',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄷㄱ','두근',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅊㅎ','축하',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅊㅊ','축하',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅆㄹㄱ','쓰레기',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄱㅇㄷ','이득',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅇㄷ','어디',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅁㄹ','몰라',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㄱㅊ','괜찮',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅅㄲ','새끼',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅁㅊ','미친',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅎㅎ+','ㅎㅎ',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅋㅋ+','ㅋㅋ',regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모음 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모음 정리\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅜ+','ㅠㅠ',regex=True)\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅠㅠ+','ㅠㅠ',regex=True)\n",
    "# 한국어, 숫자, 띄어쓰기, 하트 제외 삭제\n",
    "commentData['comment'] = commentData['comment'].str.replace('[^0-9ㅋㅎㅠ가-힣❤️ ]','',regex=True)\n",
    "## 의미가 없는 단어들로 판단하여 삭제\n",
    "com_null_idx = commentData[commentData['comment'] == ''].index\n",
    "commentData.drop(com_null_idx, inplace=True)\n",
    "commentData.reset_index(drop=True, inplace=True)\n",
    "\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅋㅋ',' ㅋㅋ ')\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅎㅎ',' ㅎㅎ ')\n",
    "commentData['comment'] = commentData['comment'].str.replace('ㅠㅠ',' ㅠㅠ ')\n",
    "commentData['comment'] = commentData['comment'].str.replace(' +',' ', regex=True)\n",
    "\n",
    "for i in range(len(commentData)):\n",
    "    if commentData.loc[i,'comment'] == '': print(i) \n",
    "    elif commentData.loc[i,'comment'] == ' ': print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentData.drop([1376,4218,11756,11895,12371,13106,13293,13922,14088,19768],inplace=True)\n",
    "commentData.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 한셀 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "for i in range(25584,len(commentData)):\n",
    "  commentData.loc[i,'comment'] = spell_checker.check(commentData.loc[i,'comment']).checked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 형태소 분석기 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash install_mecab-ko_on_colab190912.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import Spacing\n",
    "spacing = Spacing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기 후 한셀 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "original_sentence = commentData.loc[20,'comment']\n",
    "original_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*okt.morphs(original_sentence, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*kkma.morphs(original_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*komoran.morphs(original_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*mecab.morphs(original_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기\n",
    "spacing(original_sentence) # 이거는 사용x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  맞춤법 교정\n",
    "spelled_sentence = spell_checker.check(original_sentence).checked\n",
    "spelled_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*okt.morphs(spelled_sentence, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*kkma.morphs(spelled_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. mecab 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "!bash install_mecab-ko_on_colab190912.sh\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "!apt-get update -qq\n",
    "!apt-get install fonts-nanum* -qq\n",
    "import matplotlib.font_manager as fm\n",
    "sys_font = fm.findSystemFonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체언, 용언(동사, 형용사), 일반부사, 감탄사, 체언 접두사, 어근, 부호 및 숫자\n",
    "goodPos = ['NNG','NNP','NNBC','NR','NP','VV','VA','MAG','IC','XPN','XR']\n",
    "  #### 조사, 어미 등 의미없는 단어는 추출하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정식연재 토큰화\n",
    "pubCom = commentData[commentData['isPublic'] == 1]\n",
    "\n",
    "public_mecab = []\n",
    "for sentence in pubCom['comment']:\n",
    "  tokenized_sentence = mecab.pos(sentence)\n",
    "  token = []\n",
    "  for i in range(len(tokenized_sentence)):\n",
    "    if tokenized_sentence[i][1] in goodPos:\n",
    "      token.append(tokenized_sentence[i][0])\n",
    "    elif tokenized_sentence[i][1][:2] in ['VV','VA']: # 동사와/형용사 + 어미\n",
    "      token.append(tokenized_sentence[i][0])\n",
    "  public_mecab.append(token)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(public_mecab)\n",
    "wordDict = tokenizer.word_counts\n",
    "wordDict_sorted = list(sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(len(wordDict_sorted))\n",
    "print(wordDict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', background_color='white')\n",
    "gen = wc.generate_from_frequencies(wordDict)\n",
    "plt.figure()\n",
    "plt.imshow(gen)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정식연재 토큰화\n",
    "notPubCom = commentData[commentData['isPublic'] == 0]\n",
    "\n",
    "notPublic_mecab = []\n",
    "for sentence in notPubCom['comment']:\n",
    "  tokenized_sentence = mecab.pos(sentence)\n",
    "  token = []\n",
    "  for i in range(len(tokenized_sentence)):\n",
    "    if tokenized_sentence[i][1] in goodPos:\n",
    "      token.append(tokenized_sentence[i][0])\n",
    "    elif tokenized_sentence[i][1][:2] in ['VV','VA']:\n",
    "      token.append(tokenized_sentence[i][0])\n",
    "  notPublic_mecab.append(token)\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(notPublic_mecab)\n",
    "wordDict2 = tokenizer2.word_counts\n",
    "wordDict_sorted2 = list(sorted(tokenizer2.word_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "print(len(wordDict_sorted2))\n",
    "print(wordDict_sorted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf', background_color='white')\n",
    "gen = wc.generate_from_frequencies(wordDict2)\n",
    "plt.figure()\n",
    "plt.imshow(gen)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 단어 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentData2 = commentData.copy()\n",
    "\n",
    "for i in range(len(commentData)):\n",
    "  sentence = commentData.loc[i,'comment']\n",
    "  if type(sentence) == float: continue\n",
    "  tokenized_sentence = mecab.pos(sentence)\n",
    "  token = ''\n",
    "  for j in range(len(tokenized_sentence)):\n",
    "    if tokenized_sentence[j][1] in goodPos:\n",
    "      token += ' '+tokenized_sentence[j][0]\n",
    "    elif tokenized_sentence[j][1][:2] in ['VV','VA']:\n",
    "      token += ' '+tokenized_sentence[j][0]\n",
    "  commentData2.loc[i,'comment'] = token\n",
    "\n",
    "#commentData2.to_csv('comment_mecab.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = {'ㅋㅋ','좋','정식','재밌','잘','연재','네이버','다음','친구','사랑',\n",
    "             '그림','작품','빨리','감사','잼','스토리','오','ㅎㅎ','파이팅','기대',\n",
    "             '대박','궁금','남','여주','여자','데려가','축하','마음','소름','기다렸',\n",
    "             '귀엽','기다리','소원','계속','옆','시간','미쳤','존','남주','행복','늘',\n",
    "             '다시','표정','나쁜','주인공','퀄리티','귀여워','힘들','응원','힘내',\n",
    "             '현실','최고','얼굴','댓글','기억','미친','꿀','드디어','나왔','작화',\n",
    "            '굿','사이다','대단','시키','무섭','주행','생기','우와','올라가','힐링',\n",
    "            '따뜻','매주','귀여','설레','관심','이쁘','팬','리메이크','공감','소식',\n",
    "            '예쁜','추억','부들부들','귀여운','흑흑','오지','대작','쩔','모셔','탄탄',\n",
    "            '예상','담당자','반갑','가셨으면','현기증','잘생긴','세계관','완결','왜',\n",
    "            '가즈아','승격','새로','고침','귀여움','슬프','헤어지','끝나','몰입','가',\n",
    "            '답답','취향','매력','발암','깜찍','이상','싫','설정','피드백','표절','뭐',\n",
    "             '앞','내용','월','더','사람','덜','생각','베도','ㅎ','최강','도전','모르',\n",
    "            '많','소름','이번','현실','ㅋ','재미있','얼른','캐릭터','궁금','전개','꿀',\n",
    "            '처음','분위기','최고','대작','댓글','취향','존','귀여워','벌써','계속',\n",
    "            '미쳤','소재','분량','드디어','미친','기다리','작화','웃기','이뻐요','진심',\n",
    "            '느낌','두근두근','열심히','무서워','헉','올려','시작','흥미','흥미진진',\n",
    "            '어서','무섭','굿','오랜만','색감','개그','데려가','쿠키','위','예쁘',\n",
    "            '예뻐요','생각나','발암','이야기','점점','별점','재미','관심','매력',\n",
    "            '갑시다','괜찮','몰입','스타일','배경','디테일','팬','왜','귀여워','더',\n",
    "            '그리','신선','웃','연출','기분','꿈','팬','장면','비슷','따뜻','웃기','색감',\n",
    "            '성격','참신','올리','몰입','설정','꾸준히','새로운','짧','귀신','개그',\n",
    "            '주제','개그','장르','힐링','실화','명작','무섭','감정','두근두근','불쌍',\n",
    "            '신기','재미나','매주','베스트','쩔','아쉽','독특','추천','세계관','슬프',\n",
    "            '원합니다','귀여우','웃겨','감동','등록','귀염','감성','별로','알림','웃겨요',\n",
    "            '좋아하','귀여워서','개성','고퀄'}\n",
    "\n",
    "wordsList = list(wordsList)\n",
    "wordsList.sort()\n",
    "print(wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentData2['comment'] = commentData2['comment'].str.replace(' ㅋ',' ㅋㅋ')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace(' ㅎ',' ㅎㅎ')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('가셨으면','가')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('갑시다','가')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀여우','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀여운','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀여움','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀여워서','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀염','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀여워','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('귀여','귀엽')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('그리','그림')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('기다렸','기다리')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('무서워','무섭')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('미친','미쳤')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('예뻐요','예쁘')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('예쁜','예쁘')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('올라가','올려')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('올리','올려')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('웃','웃겨')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('웃겨요','웃겨')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('웃기','웃겨')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('이뻐요','예쁘')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('이쁘','예쁘')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('재미나','재미')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('재미있','재미')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('재밌','재미')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('잼','재미')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('좋','좋아하')\n",
    "commentData2['comment'] = commentData2['comment'].str.replace('흥미진진','흥미')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 댓글 내에 중복등장하는 단어 정리\n",
    "for i in range(len(commentData2)):\n",
    "    comSet = set(commentData2.loc[i,'comment'].split(' '))\n",
    "    token = ''\n",
    "    for j in range(1,len(comSet)):\n",
    "        token += ' '+list(comSet)[j]\n",
    "    commentData2.loc[i,'comment'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genreList = ['daily','comic','fantasy','action','drama','pure','sensibility','thrill','historical','sports']\n",
    "genreDict = {}\n",
    "for genre in genreList:\n",
    "    genreDict[genre] = len(commentData2[commentData2['contentGenre'].str.contains(genre)].drop_duplicates('titleId'))*6\n",
    "genreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.zeros((len(wordsList),22))\n",
    "token_ratio = pd.DataFrame(arr, columns=['public','notPublic','daily1','daily0',\n",
    "                              'comic1','comic0','fantasy1','fantasy0',\n",
    "                              'action1','action0','drama1','drama0','pure1','pure0',\n",
    "                             'sensibility1','sensibility0','thrill1','thrill0',\n",
    "                             'historical1','historical0','sports1','sports0'],\n",
    "                    index=wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubId = commentData[commentData['isPublic'] == 1].drop_duplicates('titleId')\n",
    "notPubId = commentData[commentData['isPublic'] == 0].drop_duplicates('titleId')\n",
    "print('공식연재 웹툰수: ', len(pubId))\n",
    "print('비공식연재 웹툰수: ', len(notPubId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(commentData2)):\n",
    "    tokenList = commentData2.loc[i,'comment'].split(' ')[1:]\n",
    "    for word in token_ratio.index:\n",
    "        if word in tokenList:\n",
    "            if commentData2.loc[i,'isPublic'] == 1:\n",
    "                token_ratio.loc[word,'public'] += 1/(len(pubId)*6)\n",
    "                for genre in genreList:\n",
    "                    if genre in commentData.loc[i,'contentGenre']:\n",
    "                        token_ratio.loc[word, genre + '1'] += 1/genreDict[genre]\n",
    "            elif commentData.loc[i,'isPublic'] == 0:\n",
    "                token_ratio.loc[word,'notPublic'] += 1/(len(notPubId)*6)\n",
    "                for genre in genreList:\n",
    "                    if genre in commentData.loc[i,'contentGenre']:\n",
    "                        token_ratio.loc[word, genre + '0'] += 1/genreDict[genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정식연재가 2배 많은 단어\n",
    "words1 = token_ratio[token_ratio['public'] > token_ratio['notPublic'] * 2]\n",
    "print(words1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정식연재가 2배 많은 단어\n",
    "words0 = token_ratio[token_ratio['public'] * 2 < token_ratio['notPublic']]\n",
    "print(words0.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 선택\n",
    "words0 = ['감동', '명작','비슷','새로운','소식','아쉽','오랜만','완결','이야기','짧','축하']\n",
    "words1 = ['나쁜','데려가','두근두근','등록','디테일','매주','무섭','미쳤','발암','부들부들','사이다','색감','소름','얼른','옆','오지','작화','전개','쩔','쿠키','탄탄','현실']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 변수로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleId = commentData2.drop_duplicates('titleId')['titleId']\n",
    "commentData3 = pd.DataFrame(columns= ['titleId','words0','words1'])\n",
    "commentData3['titleId'] = titleId\n",
    "commentData3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for i in range(len(commentData3)):\n",
    "  comment_dt = commentData2['comment'][commentData2['titleId'] == commentData3.loc[i,'titleId']]\n",
    "  w0 = 0; w1 = 0\n",
    "  for com in comment_dt:\n",
    "    for word in words0:\n",
    "      if word in com: w0 += 1\n",
    "    for word in words1:\n",
    "      if word in com: w1 += 1\n",
    "  commentData3.loc[i,'words0'] = w0\n",
    "  commentData3.loc[i,'words1'] = w1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commentData3.to_csv('comment_words.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
